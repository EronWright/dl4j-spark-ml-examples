{"metadata":{"name":"dl4j-iris","user_save_timestamp":"1969-12-31T16:00:00.000Z","auto_save_timestamp":"1969-12-31T16:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":["deeplearning4j:dl4j-spark-ml:0.0.3.3.5.alpha2-SNAPSHOT"],"customImports":null,"customSparkConf":{"spark.app.name":"Notebook","spark.master":"local[8]","spark.executor.memory":"1G"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Iris Classification with deeplearning4j"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val sqlContext = new org.apache.spark.sql.SQLContext(sc)\nimport sqlContext.implicits._\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.feature._\n\nimport org.deeplearning4j.spark.ml._","outputs":[{"name":"stdout","output_type":"stream","text":"sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@31b0c562\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.feature._\nimport org.deeplearning4j.spark.ml._\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":96}]},{"metadata":{},"cell_type":"markdown","source":"## Load and prepare the Iris dataset\nA sample Iris dataset is available on the local filesystem.  Here, the dataset is loaded using a DataFrame reader from `dl4j-spark-ml`.  The dataset is split into a training set and a test set."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val data = sqlContext.read\n  .format(\"org.deeplearning4j.spark.sql.sources.iris\")\n  .load(\"iris_svmLight_0.txt\")\ndata.show","outputs":[{"name":"stdout","output_type":"stream","text":"+-----+-----------------+\n|label|         features|\n+-----+-----------------+\n|  0.0|[5.1,3.5,1.4,0.2]|\n|  0.0|[5.1,3.5,1.4,0.2]|\n|  0.0|[4.9,3.0,1.4,0.2]|\n|  0.0|[4.9,3.0,1.4,0.2]|\n|  0.0|[4.7,3.2,1.3,0.2]|\n|  0.0|[4.7,3.2,1.3,0.2]|\n|  0.0|[4.6,3.1,1.5,0.2]|\n|  0.0|[4.6,3.1,1.5,0.2]|\n|  0.0|[5.0,3.6,1.4,0.2]|\n|  0.0|[5.0,3.6,1.4,0.2]|\n|  0.0|[5.4,3.9,1.7,0.4]|\n|  0.0|[5.4,3.9,1.7,0.4]|\n|  0.0|[4.6,3.4,1.4,0.3]|\n|  0.0|[4.6,3.4,1.4,0.3]|\n|  0.0|[5.0,3.4,1.5,0.2]|\n|  0.0|[5.0,3.4,1.5,0.2]|\n|  0.0|[4.4,2.9,1.4,0.2]|\n|  0.0|[4.4,2.9,1.4,0.2]|\n|  0.0|[4.9,3.1,1.5,0.1]|\n|  0.0|[4.9,3.1,1.5,0.1]|\n+-----+-----------------+\n\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":97}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val trainingData = data.sample(false, 0.6, 11L)\nval testData = data.except(trainingData)","outputs":[{"name":"stdout","output_type":"stream","text":"trainingData: org.apache.spark.sql.DataFrame = [label: double, features: vector]\ntestData: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"},{"metadata":{},"data":{"text/html":"[label: double, features: vector]"},"output_type":"execute_result","execution_count":98}]},{"metadata":{},"cell_type":"markdown","source":"## Configure an ML pipeline"},{"metadata":{},"cell_type":"markdown","source":"#### Feature scaling\nMost algorithms benefit from working with normalized feature data.  Here, Spark's `StandardScaler` normalizes the `feature` column, producing a new `scaledFeatures` column."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val scaler = new StandardScaler()\n                .setWithMean(true).setWithStd(true)\n                .setInputCol(\"features\").setOutputCol(\"scaledFeatures\");","outputs":[{"name":"stdout","output_type":"stream","text":"scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_b0fd880c6608\n"},{"metadata":{},"data":{"text/html":"stdScal_b0fd880c6608"},"output_type":"execute_result","execution_count":99}]},{"metadata":{},"cell_type":"markdown","source":"#### Neural network\nA neural network must be configured, layer-by-layer.  Here a deep-belief network (DBN) is configured to learn the Iris dataset."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.distribution.UniformDistribution\nimport org.deeplearning4j.nn.conf.layers.RBM\nimport org.deeplearning4j.nn.conf.`override`.ClassifierOverride\nimport org.deeplearning4j.nn.conf.rng.DefaultRandom\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.nd4j.linalg.lossfunctions.LossFunctions\n\nval conf = new NeuralNetConfiguration.Builder()\n                .layer(new RBM()) //the nn's layers will be RBMs\n                .nIn(4) // no. of Input nodes = 4\n                .nOut(3) // no. of Output nodes/labels = 3\n                .visibleUnit(RBM.VisibleUnit.GAUSSIAN) //Gaussian transform\n                .hiddenUnit(RBM.HiddenUnit.RECTIFIED) // Rect. Linear trans.\n                .iterations(100) // make 100 passes of guess and backprop\n                .weightInit(WeightInit.DISTRIBUTION) // initializes weights\n                .dist(new UniformDistribution(0, 1)) \n                .activationFunction(\"tanh\") // sigmoid activation of nodes\n                .k(1) // no. of times you run contrastive divergence\n                .lossFunction(LossFunctions.LossFunction.RMSE_XENT) \n                // your loss function = root-mean-squared error cross entropy\n                .learningRate(1e-1f) //the size of the steps your algo takes\n                .momentum(0.9) //a coefficient that modifies the learning rate\n                .regularization(true) // regularization fights overfitting\n                .l2(2e-4) // l2 is one type of regularization\n                .optimizationAlgo(OptimizationAlgorithm.LBFGS) \n                //optimization algorithms calculate the gradients. \n                //LBFGS is one type.\n                .constrainGradientToUnitNorm(true) \n                .list(2)\n                .hiddenLayerSizes(3) // no. of nodes in your hidden layer. \n                // this is small.\n                .`override`(1, new ClassifierOverride())\n                .build()","outputs":[{"name":"stdout","output_type":"stream","text":"import org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.distribution.UniformDistribution\nimport org.deeplearning4j.nn.conf.layers.RBM\nimport org.deeplearning4j.nn.conf.`override`.ClassifierOverride\nimport org.deeplearning4j.nn.conf.rng.DefaultRandom\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nconf: org.deeplearning4j.nn.conf.MultiLayerConfiguration = \n{\n  \"hiddenLayerSizes\" : [ 3 ],\n  \"confs\" : [ {\n    \"sparsity\" : 0.0,\n    \"useAdaGrad\" : true,\n    \"lr\" : 0.10000000149011612,\n    \"corruptionLevel\" : 0.30000001192092896,\n    \"numIterations\" : 100,\n    \"momentum\" : 0.9,\n    \"l2\" : 2.0E-4,\n    \"useRegularization\" : true,\n    \"customLossFunction\" :..."},{"metadata":{},"data":{"text/html":"{\n  &quot;hiddenLayerSizes&quot; : [ 3 ],\n  &quot;confs&quot; : [ {\n    &quot;sparsity&quot; : 0.0,\n    &quot;useAdaGrad&quot; : true,\n    &quot;lr&quot; : 0.10000000149011612,\n    &quot;corruptionLevel&quot; : 0.30000001192092896,\n    &quot;numIterations&quot; : 100,\n    &quot;momentum&quot; : 0.9,\n    &quot;l2&quot; : 2.0E-4,\n    &quot;useRegularization&quot; : true,\n    &quot;customLossFunction&quot; : null,\n    &quot;momentumAfter&quot; : null,\n    &quot;resetAdaGradIterations&quot; : -1,\n    &quot;numLineSearchIterations&quot; : 100,\n    &quot;dropOut&quot; : 0.0,\n    &quot;applySparsity&quot; : false,\n    &quot;weightInit&quot; : &quot;DISTRIBUTION&quot;,\n    &quot;optimizationAlgo&quot; : &quot;LBFGS&quot;,\n    &quot;lossFunction&quot; : &quot;RMSE_XENT&quot;,\n    &quot;constrainGradientToUnitNorm&quot; : true,\n    &quot;rng&quot; : {\n      &quot;default&quot; : {\n        &quot;seed&quot; : 1434002384624\n      }\n    },\n    &quot;dist&quot; : {\n      &quot;uniform&quot; : {\n        &quot;lower&quot; : 0.0,\n        &quot;upper&quot; : 1.0\n      }\n    },\n    &quot;stepFunction&quot; : {\n      &quot;default&quot; : { }\n    },\n    &quot;layer&quot; : {\n      &quot;RBM&quot; : { }\n    },\n    &quot;variables&quot; : [ ],\n    &quot;nIn&quot; : 4,\n    &quot;nOut&quot; : 3,\n    &quot;activationFunction&quot; : &quot;tanh&quot;,\n    &quot;visibleUnit&quot; : &quot;GAUSSIAN&quot;,\n    &quot;hiddenUnit&quot; : &quot;RECTIFIED&quot;,\n    &quot;k&quot; : 1,\n    &quot;weightShape&quot; : [ 4, 3 ],\n    &quot;filterSize&quot; : [ 2, 2, 2, 2 ],\n    &quot;stride&quot; : [ 2, 2 ],\n    &quot;kernel&quot; : 5,\n    &quot;batchSize&quot; : 100,\n    &quot;minimize&quot; : false,\n    &quot;l1&quot; : 0.0,\n    &quot;featureMapSize&quot; : [ 2, 2 ],\n    &quot;convolutionType&quot; : &quot;MAX&quot;\n  }, {\n    &quot;sparsity&quot; : 0.0,\n    &quot;useAdaGrad&quot; : true,\n    &quot;lr&quot; : 0.10000000149011612,\n    &quot;corruptionLevel&quot; : 0.30000001192092896,\n    &quot;numIterations&quot; : 100,\n    &quot;momentum&quot; : 0.9,\n    &quot;l2&quot; : 2.0E-4,\n    &quot;useRegularization&quot; : true,\n    &quot;customLossFunction&quot; : null,\n    &quot;momentumAfter&quot; : null,\n    &quot;resetAdaGradIterations&quot; : -1,\n    &quot;numLineSearchIterations&quot; : 100,\n    &quot;dropOut&quot; : 0.0,\n    &quot;applySparsity&quot; : false,\n    &quot;weightInit&quot; : &quot;ZERO&quot;,\n    &quot;optimizationAlgo&quot; : &quot;LBFGS&quot;,\n    &quot;lossFunction&quot; : &quot;MCXENT&quot;,\n    &quot;constrainGradientToUnitNorm&quot; : true,\n    &quot;rng&quot; : {\n      &quot;default&quot; : {\n        &quot;seed&quot; : 1434002384624\n      }\n    },\n    &quot;dist&quot; : {\n      &quot;uniform&quot; : {\n        &quot;lower&quot; : 0.0,\n        &quot;upper&quot; : 1.0\n      }\n    },\n    &quot;stepFunction&quot; : {\n      &quot;default&quot; : { }\n    },\n    &quot;layer&quot; : {\n      &quot;output&quot; : { }\n    },\n    &quot;variables&quot; : [ ],\n    &quot;nIn&quot; : 4,\n    &quot;nOut&quot; : 3,\n    &quot;activationFunction&quot; : &quot;softmax&quot;,\n    &quot;visibleUnit&quot; : &quot;GAUSSIAN&quot;,\n    &quot;hiddenUnit&quot; : &quot;RECTIFIED&quot;,\n    &quot;k&quot; : 1,\n    &quot;weightShape&quot; : [ 4, 3 ],\n    &quot;filterSize&quot; : [ 2, 2, 2, 2 ],\n    &quot;stride&quot; : [ 2, 2 ],\n    &quot;kernel&quot; : 5,\n    &quot;batchSize&quot; : 100,\n    &quot;minimize&quot; : false,\n    &quot;l1&quot; : 0.0,\n    &quot;featureMapSize&quot; : [ 2, 2 ],\n    &quot;convolutionType&quot; : &quot;MAX&quot;\n  } ],\n  &quot;useDropConnect&quot; : false,\n  &quot;useGaussNewtonVectorProductBackProp&quot; : false,\n  &quot;pretrain&quot; : true,\n  &quot;useRBMPropUpAsActivations&quot; : false,\n  &quot;dampingFactor&quot; : 100.0,\n  &quot;processors&quot; : { },\n  &quot;inputPreProcessors&quot; : { },\n  &quot;backward&quot; : false\n}"},"output_type":"execute_result","execution_count":100}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.deeplearning4j.spark.ml.classification.NeuralNetworkClassification\n\nval classifier = new NeuralNetworkClassification()\n                .setFeaturesCol(\"scaledFeatures\")\n                .setConf(conf)","outputs":[{"name":"stdout","output_type":"stream","text":"import org.deeplearning4j.spark.ml.classification.NeuralNetworkClassification\nclassifier: org.deeplearning4j.spark.ml.classification.NeuralNetworkClassification = nnClassification_ae9a057d2438\n"},{"metadata":{},"data":{"text/html":"nnClassification_ae9a057d2438"},"output_type":"execute_result","execution_count":101}]},{"metadata":{},"cell_type":"markdown","source":"#### Pipeline assembly\nAn overall ML pipeline is assembled."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val pipeline = new Pipeline()\n                .setStages(Array(scaler, classifier))","outputs":[{"name":"stdout","output_type":"stream","text":"pipeline: org.apache.spark.ml.Pipeline = pipeline_51116890801b\n"},{"metadata":{},"data":{"text/html":"pipeline_51116890801b"},"output_type":"execute_result","execution_count":102}]},{"metadata":{},"cell_type":"markdown","source":"## Train and test the neural network"},{"metadata":{},"cell_type":"markdown","source":"#### Train\nThe pipeline fits a model to the training data using the neural network."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val model = pipeline.fit(trainingData)","outputs":[{"name":"stdout","output_type":"stream","text":"model: org.apache.spark.ml.PipelineModel = pipeline_51116890801b\n"},{"metadata":{},"data":{"text/html":"pipeline_51116890801b"},"output_type":"execute_result","execution_count":107}]},{"metadata":{},"cell_type":"markdown","source":"#### Test\nThe trained model is used to make predictions about the Iris test data.  Here the model produces a new column called `predictions`."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val predictions = model.transform(trainingData)\n\npredictions.select($\"features\", $\"label\" as \"actual\", $\"prediction\").show(100)","outputs":[{"name":"stdout","output_type":"stream","text":"+-----------------+------+----------+\n|         features|actual|prediction|\n+-----------------+------+----------+\n|[5.1,3.5,1.4,0.2]|   0.0|       0.0|\n|[5.1,3.5,1.4,0.2]|   0.0|       0.0|\n|[4.9,3.0,1.4,0.2]|   0.0|       0.0|\n|[4.9,3.0,1.4,0.2]|   0.0|       0.0|\n|[4.7,3.2,1.3,0.2]|   0.0|       0.0|\n|[4.7,3.2,1.3,0.2]|   0.0|       0.0|\n|[4.6,3.1,1.5,0.2]|   0.0|       0.0|\n|[4.6,3.1,1.5,0.2]|   0.0|       0.0|\n|[5.0,3.6,1.4,0.2]|   0.0|       0.0|\n|[5.4,3.9,1.7,0.4]|   0.0|       0.0|\n|[4.6,3.4,1.4,0.3]|   0.0|       0.0|\n|[4.6,3.4,1.4,0.3]|   0.0|       0.0|\n|[5.0,3.4,1.5,0.2]|   0.0|       0.0|\n|[4.4,2.9,1.4,0.2]|   0.0|       0.0|\n|[4.9,3.1,1.5,0.1]|   0.0|       0.0|\n|[5.4,3.7,1.5,0.2]|   0.0|       0.0|\n|[4.8,3.4,1.6,0.2]|   0.0|       0.0|\n|[4.8,3.0,1.4,0.1]|   0.0|       0.0|\n|[4.3,3.0,1.1,0.1]|   0.0|       0.0|\n|[5.8,4.0,1.2,0.2]|   0.0|       0.0|\n|[5.7,4.4,1.5,0.4]|   0.0|       0.0|\n|[5.4,3.9,1.3,0.4]|   0.0|       0.0|\n|[5.1,3.5,1.4,0.3]|   0.0|       0.0|\n|[5.7,3.8,1.7,0.3]|   0.0|       0.0|\n|[5.7,3.8,1.7,0.3]|   0.0|       0.0|\n|[5.1,3.8,1.5,0.3]|   0.0|       0.0|\n|[4.6,3.6,1.0,0.2]|   0.0|       0.0|\n|[5.1,3.3,1.7,0.5]|   0.0|       0.0|\n|[5.1,3.3,1.7,0.5]|   0.0|       0.0|\n|[4.8,3.4,1.9,0.2]|   0.0|       0.0|\n|[5.0,3.0,1.6,0.2]|   0.0|       0.0|\n|[5.0,3.4,1.6,0.4]|   0.0|       0.0|\n|[5.0,3.4,1.6,0.4]|   0.0|       0.0|\n|[5.2,3.5,1.5,0.2]|   0.0|       0.0|\n|[5.2,3.4,1.4,0.2]|   0.0|       0.0|\n|[5.2,3.4,1.4,0.2]|   0.0|       0.0|\n|[4.7,3.2,1.6,0.2]|   0.0|       0.0|\n|[4.8,3.1,1.6,0.2]|   0.0|       0.0|\n|[4.8,3.1,1.6,0.2]|   0.0|       0.0|\n|[5.4,3.4,1.5,0.4]|   0.0|       0.0|\n|[5.4,3.4,1.5,0.4]|   0.0|       0.0|\n|[5.5,4.2,1.4,0.2]|   0.0|       0.0|\n|[5.5,4.2,1.4,0.2]|   0.0|       0.0|\n|[4.9,3.1,1.5,0.1]|   0.0|       0.0|\n|[5.0,3.2,1.2,0.2]|   0.0|       0.0|\n|[5.5,3.5,1.3,0.2]|   0.0|       0.0|\n|[4.9,3.1,1.5,0.1]|   0.0|       0.0|\n|[4.9,3.1,1.5,0.1]|   0.0|       0.0|\n|[4.4,3.0,1.3,0.2]|   0.0|       0.0|\n|[4.4,3.0,1.3,0.2]|   0.0|       0.0|\n|[5.1,3.4,1.5,0.2]|   0.0|       0.0|\n|[4.5,2.3,1.3,0.3]|   0.0|       0.0|\n|[4.5,2.3,1.3,0.3]|   0.0|       0.0|\n|[4.4,3.2,1.3,0.2]|   0.0|       0.0|\n|[5.0,3.5,1.6,0.6]|   0.0|       0.0|\n|[5.1,3.8,1.9,0.4]|   0.0|       0.0|\n|[5.1,3.8,1.9,0.4]|   0.0|       0.0|\n|[4.6,3.2,1.4,0.2]|   0.0|       0.0|\n|[5.3,3.7,1.5,0.2]|   0.0|       0.0|\n|[5.3,3.7,1.5,0.2]|   0.0|       0.0|\n|[5.0,3.3,1.4,0.2]|   0.0|       0.0|\n|[6.4,3.2,4.5,1.5]|   1.0|       1.0|\n|[6.9,3.1,4.9,1.5]|   1.0|       1.0|\n|[6.9,3.1,4.9,1.5]|   1.0|       1.0|\n|[5.5,2.3,4.0,1.3]|   1.0|       0.0|\n|[5.5,2.3,4.0,1.3]|   1.0|       0.0|\n|[6.5,2.8,4.6,1.5]|   1.0|       0.0|\n|[5.7,2.8,4.5,1.3]|   1.0|       0.0|\n|[5.7,2.8,4.5,1.3]|   1.0|       0.0|\n|[6.3,3.3,4.7,1.6]|   1.0|       1.0|\n|[6.3,3.3,4.7,1.6]|   1.0|       1.0|\n|[4.9,2.4,3.3,1.0]|   1.0|       0.0|\n|[4.9,2.4,3.3,1.0]|   1.0|       0.0|\n|[6.6,2.9,4.6,1.3]|   1.0|       0.0|\n|[5.9,3.0,4.2,1.5]|   1.0|       0.0|\n|[6.0,2.2,4.0,1.0]|   1.0|       0.0|\n|[6.0,2.2,4.0,1.0]|   1.0|       0.0|\n|[6.1,2.9,4.7,1.4]|   1.0|       0.0|\n|[5.6,2.9,3.6,1.3]|   1.0|       0.0|\n|[5.6,2.9,3.6,1.3]|   1.0|       0.0|\n|[6.7,3.1,4.4,1.4]|   1.0|       0.0|\n|[5.6,3.0,4.5,1.5]|   1.0|       0.0|\n|[6.2,2.2,4.5,1.5]|   1.0|       0.0|\n|[5.6,2.5,3.9,1.1]|   1.0|       0.0|\n|[5.9,3.2,4.8,1.8]|   1.0|       2.0|\n|[5.9,3.2,4.8,1.8]|   1.0|       2.0|\n|[6.1,2.8,4.0,1.3]|   1.0|       0.0|\n|[6.3,2.5,4.9,1.5]|   1.0|       0.0|\n|[6.3,2.5,4.9,1.5]|   1.0|       0.0|\n|[6.1,2.8,4.7,1.2]|   1.0|       0.0|\n|[6.1,2.8,4.7,1.2]|   1.0|       0.0|\n|[6.4,2.9,4.3,1.3]|   1.0|       0.0|\n|[6.4,2.9,4.3,1.3]|   1.0|       0.0|\n|[6.6,3.0,4.4,1.4]|   1.0|       0.0|\n|[6.6,3.0,4.4,1.4]|   1.0|       0.0|\n|[6.7,3.0,5.0,1.7]|   1.0|       1.0|\n|[6.7,3.0,5.0,1.7]|   1.0|       1.0|\n|[6.0,2.9,4.5,1.5]|   1.0|       0.0|\n|[6.0,2.9,4.5,1.5]|   1.0|       0.0|\n|[5.7,2.6,3.5,1.0]|   1.0|       0.0|\n+-----------------+------+----------+\n\npredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector, scaledFeatures: vector, rawPrediction: vector, prediction: double]\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":108}]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate\nSpark ML provides an evaluation framework including cross-fit validation for hyper-parameter tuning.  Here we simply use an evaluator to calculate the rmse."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val rootMeanSquaredError = new RegressionEvaluator().evaluate(predictions)","outputs":[{"name":"stdout","output_type":"stream","text":"rootMeanSquaredError: Double = 0.8855566194861791\n"},{"metadata":{},"data":{"text/html":"0.8855566194861791"},"output_type":"execute_result","execution_count":109}]}],"nbformat":4}